{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b55ba97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "english_texts = [\"i love you\", \"how are you\", \"good morning\", \"thank you\", \"see you soon\"]\n",
    "french_texts  = [\"je t'aime\", \"comment ça va\", \"bonjour\", \"merci\", \"à bientôt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71cc6070",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer_fr = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer_en.fit_on_texts(english_texts)\n",
    "tokenizer_fr.fit_on_texts(french_texts)\n",
    "\n",
    "sequences_en = tokenizer_en.texts_to_sequences(english_texts)\n",
    "sequences_fr = tokenizer_fr.texts_to_sequences(french_texts)\n",
    "\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(sequences_en, padding='post')\n",
    "y = tf.keras.preprocessing.sequence.pad_sequences(sequences_fr, padding='post')\n",
    "\n",
    "vocab_inp = len(tokenizer_en.word_index) + 1\n",
    "vocab_tar = len(tokenizer_fr.word_index) + 1\n",
    "embed_dim = 64\n",
    "num_heads = 2\n",
    "ff_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c69d7828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f91dcf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(X.shape[1],))\n",
    "embedding = layers.Embedding(vocab_inp, embed_dim)(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)(embedding)\n",
    "flatten = layers.GlobalAveragePooling1D()(transformer_block)\n",
    "outputs = layers.Dense(vocab_tar, activation=\"softmax\")(flatten)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a90f4440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19a52350050>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(X, y[:, 0], epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ed3bb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Predicted French word: merci\n"
     ]
    }
   ],
   "source": [
    "test_input = tokenizer_en.texts_to_sequences([\"Thank you\"])\n",
    "test_input = tf.keras.preprocessing.sequence.pad_sequences(test_input, maxlen=X.shape[1], padding='post')\n",
    "pred = model.predict(test_input)\n",
    "pred_word_index = tf.argmax(pred[0]).numpy()\n",
    "\n",
    "for word, index in tokenizer_fr.word_index.items():\n",
    "    if index == pred_word_index:\n",
    "        print(\"Predicted French word:\", word)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d88be8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
