{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16238025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e1e229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
    "SEQ_LENGTH = 10          \n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "EMBEDDING_DIM = 128\n",
    "RNN_UNITS = 256\n",
    "EPOCHS = 5               \n",
    "MODEL_TYPE = \"bidir_lstm\"  \n",
    "CHECKPOINT_DIR = \"./nw_checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9badcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "\u001b[1m1115394/1115394\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Loaded text length: 1115394 chars\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\"dataset.txt\", DATA_URL)\n",
    "with open(path_to_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Loaded text length: {len(text)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6260d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size (including OOV): 12634\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts([text])         \n",
    "\n",
    "token_sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "vocab_size = len(tokenizer.word_index) + 1  \n",
    "print(f\"Vocab size (including OOV): {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb2bb9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared. Sample batch shapes:\n",
      "x: (64, 10) y: (64,)\n"
     ]
    }
   ],
   "source": [
    "tokens_ds = tf.data.Dataset.from_tensor_slices(token_sequence)\n",
    "windows = tokens_ds.batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_seq = chunk[:-1]  \n",
    "    target = chunk[-1]       \n",
    "    return input_seq, target\n",
    "\n",
    "dataset = windows.map(split_input_target, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Dataset prepared. Sample batch shapes:\")\n",
    "for x, y in dataset.take(1):\n",
    "    print(\"x:\", x.shape, \"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5b05024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bluepal\\anaconda3\\envs\\python_course\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,617,152</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">788,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12634</span>)          │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,481,242</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │     \u001b[38;5;34m1,617,152\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m788,480\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12634\u001b[0m)          │     \u001b[38;5;34m6,481,242\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,886,874</span> (33.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,886,874\u001b[0m (33.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,886,874</span> (33.90 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,886,874\u001b[0m (33.90 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_training_model(vocab_size, embedding_dim, rnn_units, seq_length, model_type=\"bidir_lstm\"):\n",
    "   \n",
    "    inputs = tf.keras.Input(shape=(seq_length,), dtype=\"int32\")\n",
    "    x = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length)(inputs)\n",
    "\n",
    "    if model_type == \"lstm\":\n",
    "        x = tf.keras.layers.LSTM(rnn_units)(x)\n",
    "    elif model_type == \"gru\":\n",
    "        x = tf.keras.layers.GRU(rnn_units)(x)\n",
    "    elif model_type == \"bidir_lstm\":\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units))(x)\n",
    "    elif model_type == \"bidir_gru\":\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_units))(x)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model_type\")\n",
    "\n",
    "    logits = tf.keras.layers.Dense(vocab_size)(x)  \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n",
    "\n",
    "model = build_training_model(vocab_size, EMBEDDING_DIM, RNN_UNITS, SEQ_LENGTH, model_type=MODEL_TYPE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bddb28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn)\n",
    "\n",
    "checkpoint_pattern = os.path.join(CHECKPOINT_DIR, \"nw_ckpt_epoch_{epoch:02d}.weights.h5\")\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_pattern,\n",
    "    save_weights_only=True,\n",
    "    save_freq=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "542161c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 251ms/step - loss: 7.2551\n",
      "Epoch 2/5\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 253ms/step - loss: 6.5378\n",
      "Epoch 3/5\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 261ms/step - loss: 6.3044\n",
      "Epoch 4/5\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 256ms/step - loss: 6.0134\n",
      "Epoch 5/5\n",
      "\u001b[1m289/289\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 214ms/step - loss: 5.6354\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c57ff351",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = {index: word for word, index in tokenizer.word_index.items()}\n",
    "index_word[0] = \"\"  \n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def predict_next_words(model, tokenizer, seed_text, seq_length=SEQ_LENGTH, top_k=5, temperature=1.0):\n",
    "    \n",
    "    seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    seq = seq[-seq_length:]\n",
    "    input_seq = pad_sequences([seq], maxlen=seq_length, padding=\"pre\")  \n",
    "    logits = model.predict(input_seq, verbose=0) \n",
    "    logits = logits[0]  \n",
    "\n",
    "    if temperature != 1.0:\n",
    "        logits = logits / (temperature + 1e-8)\n",
    "\n",
    "    probs = tf.nn.softmax(logits).numpy()\n",
    "\n",
    "    top_k = min(top_k, len(probs))\n",
    "    top_indices = np.argpartition(probs, -top_k)[-top_k:]\n",
    "    top_indices = top_indices[np.argsort(-probs[top_indices])]  \n",
    "    results = [(index_word.get(int(i), \"<UNK>\"), float(probs[i])) for i in top_indices]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "042ba51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_continuation(model, tokenizer, seed_text, num_words=20, seq_length=SEQ_LENGTH,\n",
    "                          temperature=1.0, sample=True):\n",
    "   \n",
    "    current = seed_text.strip()\n",
    "    for _ in range(num_words):\n",
    "        seq = tokenizer.texts_to_sequences([current])[0]\n",
    "        seq = seq[-seq_length:]\n",
    "        input_seq = pad_sequences([seq], maxlen=seq_length, padding=\"pre\")\n",
    "        logits = model.predict(input_seq, verbose=0)[0]\n",
    "        logits = logits / (temperature + 1e-8)\n",
    "        probs = tf.nn.softmax(logits).numpy()\n",
    "\n",
    "        if sample:\n",
    "            next_id = np.random.choice(len(probs), p=probs)\n",
    "        else:\n",
    "            next_id = int(np.argmax(probs))\n",
    "\n",
    "        next_word = index_word.get(next_id, \"<UNK>\")\n",
    "        if next_word == \"\":\n",
    "            break\n",
    "        current = current + \" \" + next_word\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04fa6cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from: ./nw_checkpoints\\nw_ckpt_epoch_05.weights.h5\n"
     ]
    }
   ],
   "source": [
    "weights_files = glob.glob(os.path.join(CHECKPOINT_DIR, \"*.weights.h5\"))\n",
    "weights_files.sort()\n",
    "if len(weights_files) > 0:\n",
    "    latest = weights_files[-1]\n",
    "    print(\"Loading weights from:\", latest)\n",
    "    model.load_weights(latest)\n",
    "else:\n",
    "    print(\"No checkpoint weights found (you can skip loading if you just trained).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fd00fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 next word predictions (seed: 'to be or not'):\n",
      "[('which', 0.01709016226232052), ('nothing', 0.014554977416992188), ('day', 0.011925100348889828), ('my', 0.010046116076409817), ('her', 0.009882175363600254)]\n",
      "\n",
      "Generated continuation (sample=true):\n",
      "to be or not her day which you by escalus put be himself it you may be were my great friend king vincentio not show'd leicestershire autolycus the admired as hastings with ourselves and\n",
      "\n",
      "Generated continuation (greedy):\n",
      "to be or not which the day of the day of his head of his head of his head of his head of his head of his head of his head of his head\n"
     ]
    }
   ],
   "source": [
    "seed = \"to be or not\"\n",
    "print(\"\\nTop 5 next word predictions (seed: '%s'):\" % seed)\n",
    "print(predict_next_words(model, tokenizer, seed, top_k=5, temperature=0.8))\n",
    "\n",
    "print(\"\\nGenerated continuation (sample=true):\")\n",
    "print(generate_continuation(model, tokenizer, seed, num_words=30, temperature=0.8, sample=True))\n",
    "\n",
    "print(\"\\nGenerated continuation (greedy):\")\n",
    "print(generate_continuation(model, tokenizer, seed, num_words=30, temperature=0.8, sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22d977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
