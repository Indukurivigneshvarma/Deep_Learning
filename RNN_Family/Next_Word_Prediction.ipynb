{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16238025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e1e229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
    "SEQ_LENGTH = 20         \n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 512\n",
    "EPOCHS = 20               \n",
    "MODEL_TYPE = \"bidir_lstm\"  \n",
    "CHECKPOINT_DIR = \"./nw_checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae9badcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text length: 1115394 chars\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\"dataset.txt\", DATA_URL)\n",
    "with open(path_to_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Loaded text length: {len(text)} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6260d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size (including OOV): 12634\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts([text])         \n",
    "\n",
    "token_sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "vocab_size = len(tokenizer.word_index) + 1  \n",
    "print(f\"Vocab size (including OOV): {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb2bb9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared. Sample batch shapes:\n",
      "x: (64, 20) y: (64,)\n"
     ]
    }
   ],
   "source": [
    "tokens_ds = tf.data.Dataset.from_tensor_slices(token_sequence)\n",
    "windows = tokens_ds.batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(chunk):\n",
    "    input_seq = chunk[:-1]  \n",
    "    target = chunk[-1]       \n",
    "    return input_seq, target\n",
    "\n",
    "dataset = windows.map(split_input_target, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Dataset prepared. Sample batch shapes:\")\n",
    "for x, y in dataset.take(1):\n",
    "    print(\"x:\", x.shape, \"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5b05024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,234,304</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,149,824</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12634</span>)          │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,949,850</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │     \u001b[38;5;34m3,234,304\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m3,149,824\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12634\u001b[0m)          │    \u001b[38;5;34m12,949,850\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,333,978</span> (73.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,333,978\u001b[0m (73.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,333,978</span> (73.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,333,978\u001b[0m (73.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_training_model(vocab_size, embedding_dim, rnn_units, seq_length, model_type=\"bidir_lstm\"):\n",
    "   \n",
    "    inputs = tf.keras.Input(shape=(seq_length,), dtype=\"int32\")\n",
    "    x = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_length)(inputs)\n",
    "\n",
    "    if model_type == \"lstm\":\n",
    "        x = tf.keras.layers.LSTM(rnn_units)(x)\n",
    "    elif model_type == \"gru\":\n",
    "        x = tf.keras.layers.GRU(rnn_units)(x)\n",
    "    elif model_type == \"bidir_lstm\":\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units))(x)\n",
    "    elif model_type == \"bidir_gru\":\n",
    "        x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_units))(x)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model_type\")\n",
    "\n",
    "    logits = tf.keras.layers.Dense(vocab_size)(x)  \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n",
    "\n",
    "model = build_training_model(vocab_size, EMBEDDING_DIM, RNN_UNITS, SEQ_LENGTH, model_type=MODEL_TYPE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bddb28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn)\n",
    "\n",
    "checkpoint_pattern = os.path.join(CHECKPOINT_DIR, \"nw_ckpt_epoch_{epoch:02d}.weights.h5\")\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_pattern,\n",
    "    save_weights_only=True,\n",
    "    save_freq=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "542161c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 875ms/step - loss: 7.4059\n",
      "Epoch 2/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 879ms/step - loss: 6.5195\n",
      "Epoch 3/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 740ms/step - loss: 6.2139\n",
      "Epoch 4/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 702ms/step - loss: 5.9270\n",
      "Epoch 5/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 857ms/step - loss: 5.6232\n",
      "Epoch 6/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 829ms/step - loss: 5.2694\n",
      "Epoch 7/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 736ms/step - loss: 4.8055\n",
      "Epoch 8/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 711ms/step - loss: 4.2421\n",
      "Epoch 9/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 706ms/step - loss: 3.5670\n",
      "Epoch 10/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 719ms/step - loss: 2.8335\n",
      "Epoch 11/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 762ms/step - loss: 2.1317\n",
      "Epoch 12/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 705ms/step - loss: 1.5276\n",
      "Epoch 13/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 736ms/step - loss: 1.0458\n",
      "Epoch 14/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 745ms/step - loss: 0.6776\n",
      "Epoch 15/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 656ms/step - loss: 0.4196\n",
      "Epoch 16/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 663ms/step - loss: 0.2377\n",
      "Epoch 17/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 713ms/step - loss: 0.1338\n",
      "Epoch 18/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 691ms/step - loss: 0.0786\n",
      "Epoch 19/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 714ms/step - loss: 0.0497\n",
      "Epoch 20/20\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 813ms/step - loss: 0.0354\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c57ff351",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = {index: word for word, index in tokenizer.word_index.items()}\n",
    "index_word[0] = \"\"  \n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def predict_next_words(model, tokenizer, seed_text, seq_length=SEQ_LENGTH, top_k=5, temperature=1.0):\n",
    "    \n",
    "    seq = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    seq = seq[-seq_length:]\n",
    "    input_seq = pad_sequences([seq], maxlen=seq_length, padding=\"pre\")  \n",
    "    logits = model.predict(input_seq, verbose=0) \n",
    "    logits = logits[0]  \n",
    "\n",
    "    if temperature != 1.0:\n",
    "        logits = logits / (temperature + 1e-8)\n",
    "\n",
    "    probs = tf.nn.softmax(logits).numpy()\n",
    "\n",
    "    top_k = min(top_k, len(probs))\n",
    "    top_indices = np.argpartition(probs, -top_k)[-top_k:]\n",
    "    top_indices = top_indices[np.argsort(-probs[top_indices])]  \n",
    "    results = [(index_word.get(int(i), \"<UNK>\"), float(probs[i])) for i in top_indices]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "042ba51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_continuation(model, tokenizer, seed_text, num_words=20, seq_length=SEQ_LENGTH,\n",
    "                          temperature=1.0, sample=True):\n",
    "   \n",
    "    current = seed_text.strip()\n",
    "    for _ in range(num_words):\n",
    "        seq = tokenizer.texts_to_sequences([current])[0]\n",
    "        seq = seq[-seq_length:]\n",
    "        input_seq = pad_sequences([seq], maxlen=seq_length, padding=\"pre\")\n",
    "        logits = model.predict(input_seq, verbose=0)[0]\n",
    "        logits = logits / (temperature + 1e-8)\n",
    "        probs = tf.nn.softmax(logits).numpy()\n",
    "\n",
    "        if sample:\n",
    "            next_id = np.random.choice(len(probs), p=probs)\n",
    "        else:\n",
    "            next_id = int(np.argmax(probs))\n",
    "\n",
    "        next_word = index_word.get(next_id, \"<UNK>\")\n",
    "        if next_word == \"\":\n",
    "            break\n",
    "        current = current + \" \" + next_word\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04fa6cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from: ./nw_checkpoints\\nw_ckpt_epoch_20.weights.h5\n"
     ]
    }
   ],
   "source": [
    "weights_files = glob.glob(os.path.join(CHECKPOINT_DIR, \"*.weights.h5\"))\n",
    "weights_files.sort()\n",
    "if len(weights_files) > 0:\n",
    "    latest = weights_files[-1]\n",
    "    print(\"Loading weights from:\", latest)\n",
    "    model.load_weights(latest)\n",
    "else:\n",
    "    print(\"No checkpoint weights found (you can skip loading if you just trained).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fd00fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 next word predictions (seed: 'to be or not'):\n",
      "[('out', 0.7006934285163879), ('when', 0.1601499617099762), ('faith', 0.014758005738258362), ('it', 0.013456449843943119), ('in', 0.011435184627771378)]\n",
      "\n",
      "Generated continuation (sample=true):\n",
      "to be or not out of our our men are out than out it ay you think the happy will now he hatred so cast so resign that up is he he autolycus your\n",
      "\n",
      "Generated continuation (greedy):\n",
      "to be or not out when our faith are faith with with dear make die than if i think but biondello the lady's and believe to live that now you should the letter is\n"
     ]
    }
   ],
   "source": [
    "seed = \"to be or not\"\n",
    "print(\"\\nTop 5 next word predictions (seed: '%s'):\" % seed)\n",
    "print(predict_next_words(model, tokenizer, seed, top_k=5, temperature=0.8))\n",
    "\n",
    "print(\"\\nGenerated continuation (sample=true):\")\n",
    "print(generate_continuation(model, tokenizer, seed, num_words=30, temperature=0.8, sample=True))\n",
    "\n",
    "print(\"\\nGenerated continuation (greedy):\")\n",
    "print(generate_continuation(model, tokenizer, seed, num_words=30, temperature=0.8, sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22d977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
